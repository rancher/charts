--- charts-original/values.yaml
+++ charts/values.yaml
@@ -2,13 +2,635 @@
 # This is a YAML-formatted file.
 # Declare variables to be passed into your templates.
 
+# Rancher Monitoring Configuration
+
+## Configuration for prometheus-adapter
+## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter
+##
+prometheus-adapter:
+  enabled: true
+  prometheus:
+    # Change this if you change the namespaceOverride or nameOverride of prometheus-operator
+    url: http://rancher-monitoring-prometheus.cattle-monitoring-system.svc
+    port: 9090
+
+## RKE PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
+##
+rkeControllerManager:
+  enabled: false
+  metricsPort: 10257 # default to secure port as of k8s >= 1.22
+  component: kube-controller-manager
+  clients:
+    https:
+      enabled: true
+      insecureSkipVerify: true
+      useServiceAccountCredentials: true
+    port: 10011
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/controlplane: "true"
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+  kubeVersionOverrides:
+  - constraint: "< 1.22"
+    values:
+      metricsPort: 10252 # default to insecure port in k8s < 1.22
+      clients:
+        https:
+          enabled: false
+          insecureSkipVerify: false
+          useServiceAccountCredentials: false
+
+rkeScheduler:
+  enabled: false
+  metricsPort: 10259
+  component: kube-scheduler
+  clients:
+    https:
+      enabled: true
+      insecureSkipVerify: true
+      useServiceAccountCredentials: true
+    port: 10012
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/controlplane: "true"
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+  kubeVersionOverrides:
+  - constraint: "< 1.23"
+    values:
+      metricsPort: 10251 # default to insecure port in k8s < 1.23
+      clients:
+        https:
+          enabled: false
+          insecureSkipVerify: false
+          useServiceAccountCredentials: false
+
+rkeProxy:
+  enabled: false
+  metricsPort: 10249
+  component: kube-proxy
+  clients:
+    port: 10013
+    useLocalhost: true
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+rkeEtcd:
+  enabled: false
+  metricsPort: 2379
+  component: kube-etcd
+  clients:
+    port: 10014
+    https:
+      enabled: true
+      certDir: /etc/kubernetes/ssl
+      certFile: kube-etcd-*.pem
+      keyFile: kube-etcd-*-key.pem
+      caCertFile: kube-ca.pem
+      seLinuxOptions:
+        # Gives rkeEtcd permissions to read files in /etc/kubernetes/*
+        # Type is defined in https://github.com/rancher/rancher-selinux
+        type: rke_kubereader_t
+    nodeSelector:
+      node-role.kubernetes.io/etcd: "true"
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+rkeIngressNginx:
+  enabled: false
+  metricsPort: 10254
+  component: ingress-nginx
+  clients:
+    port: 10015
+    useLocalhost: true
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+    nodeSelector:
+      node-role.kubernetes.io/worker: "true"
+
+## k3s PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
+##
+k3sServer:
+  enabled: false
+  metricsPort: 10250
+  component: k3s-server
+  clients:
+    port: 10013
+    useLocalhost: true
+    https:
+      enabled: true
+      useServiceAccountCredentials: true
+      insecureSkipVerify: true
+    rbac:
+      additionalRules:
+      - nonResourceURLs: ["/metrics/cadvisor"]
+        verbs: ["get"]
+      - apiGroups: [""]
+        resources: ["nodes/metrics"]
+        verbs: ["get"]
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+  serviceMonitor:
+    endpoints:
+    - port: metrics
+      honorLabels: true
+      relabelings:
+      - sourceLabels: [__metrics_path__]
+        targetLabel: metrics_path
+    - port: metrics
+      path: /metrics/cadvisor
+      honorLabels: true
+      relabelings:
+      - sourceLabels: [__metrics_path__]
+        targetLabel: metrics_path
+    - port: metrics
+      path: /metrics/probes
+      honorLabels: true
+      relabelings:
+      - sourceLabels: [__metrics_path__]
+        targetLabel: metrics_path
+
+hardened:
+  k3s:  
+    networkPolicy:
+      enabled: true
+
+## KubeADM PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
+##
+kubeAdmControllerManager:
+  enabled: false
+  metricsPort: 10257
+  component: kube-controller-manager
+  clients:
+    port: 10011
+    useLocalhost: true
+    https:
+      enabled: true
+      useServiceAccountCredentials: true
+      insecureSkipVerify: true
+    nodeSelector:
+      node-role.kubernetes.io/master: ""
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+kubeAdmScheduler:
+  enabled: false
+  metricsPort: 10259
+  component: kube-scheduler
+  clients:
+    port: 10012
+    useLocalhost: true
+    https:
+      enabled: true
+      useServiceAccountCredentials: true
+      insecureSkipVerify: true
+    nodeSelector:
+      node-role.kubernetes.io/master: ""
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+kubeAdmProxy:
+  enabled: false
+  metricsPort: 10249
+  component: kube-proxy
+  clients:
+    port: 10013
+    useLocalhost: true
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+kubeAdmEtcd:
+  enabled: false
+  metricsPort: 2381
+  component: kube-etcd
+  clients:
+    port: 10014
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/master: ""
+    tolerations:
+    - effect: "NoExecute"
+      operator: "Exists"
+    - effect: "NoSchedule"
+      operator: "Exists"
+
+## rke2 PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
+##
+rke2ControllerManager:
+  enabled: false
+  metricsPort: 10257 # default to secure port as of k8s >= 1.22
+  component: kube-controller-manager
+  clients:
+    https:
+      enabled: true
+      insecureSkipVerify: true
+      useServiceAccountCredentials: true
+    port: 10011
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/master: "true"
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+  kubeVersionOverrides:
+  - constraint: "< 1.22"
+    values:
+      metricsPort: 10252 # default to insecure port in k8s < 1.22
+      clients:
+        https:
+          enabled: false
+          insecureSkipVerify: false
+          useServiceAccountCredentials: false
+
+rke2Scheduler:
+  enabled: false
+  metricsPort: 10259 # default to secure port as of k8s >= 1.22
+  component: kube-scheduler
+  clients:
+    https:
+      enabled: true
+      insecureSkipVerify: true
+      useServiceAccountCredentials: true
+    port: 10012
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/master: "true"
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+  kubeVersionOverrides:
+  - constraint: "< 1.22"
+    values:
+      metricsPort: 10251 # default to insecure port in k8s < 1.22
+      clients:
+        https:
+          enabled: false
+          insecureSkipVerify: false
+          useServiceAccountCredentials: false
+
+rke2Proxy:
+  enabled: false
+  metricsPort: 10249
+  component: kube-proxy
+  clients:
+    port: 10013
+    useLocalhost: true
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+
+rke2Etcd:
+  enabled: false
+  metricsPort: 2381
+  component: kube-etcd
+  clients:
+    port: 10014
+    useLocalhost: true
+    nodeSelector:
+      node-role.kubernetes.io/etcd: "true"
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+
+rke2IngressNginx:
+  enabled: false
+  metricsPort: 10254
+  component: ingress-nginx
+  # in the RKE2 cluster, the ingress-nginx-controller is deployed
+  # as a non-hostNetwork workload starting at the following versions
+  # - >= v1.22.12+rke2r1 < 1.23.0-0
+  # - >= v1.23.9+rke2r1 < 1.24.0-0
+  # - >= v1.24.3+rke2r1 < 1.25.0-0
+  # - >= v1.25.0+rke2r1
+  # As a result we do not need clients and proxies as we can directly create
+  # a service that targets the workload with the given app name
+  namespaceOverride: kube-system
+  clients:
+    enabled: false
+  proxy:
+    enabled: false
+  service:
+    selector:
+      app.kubernetes.io/name: rke2-ingress-nginx
+  kubeVersionOverrides:
+  - constraint: "< 1.21.0-0"
+    values:
+      namespaceOverride: ""
+      clients:
+        enabled: true
+        port: 10015
+        useLocalhost: true
+        tolerations:
+          - effect: "NoExecute"
+            operator: "Exists"
+          - effect: "NoSchedule"
+            operator: "Exists"
+        affinity:
+          podAffinity:
+            requiredDuringSchedulingIgnoredDuringExecution:
+              - labelSelector:
+                  matchExpressions:
+                    - key: "app.kubernetes.io/component"
+                      operator: "In"
+                      values:
+                        - "controller"
+                topologyKey: "kubernetes.io/hostname"
+                namespaces:
+                  - "kube-system"
+        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
+        # a DaemonSet with 1 pod when RKE2 version is < 1.21.0-0
+        deployment:
+          enabled: false
+      proxy:
+        enabled: true
+      service:
+        selector: false
+  - constraint: ">= 1.21.0-0 < 1.22.12-0"
+    values:
+      namespaceOverride: ""
+      clients:
+        enabled: true
+        port: 10015
+        useLocalhost: true
+        tolerations:
+          - effect: "NoExecute"
+            operator: "Exists"
+          - effect: "NoSchedule"
+            operator: "Exists"
+        affinity:
+          podAffinity:
+            requiredDuringSchedulingIgnoredDuringExecution:
+              - labelSelector:
+                  matchExpressions:
+                    - key: "app.kubernetes.io/component"
+                      operator: "In"
+                      values:
+                        - "controller"
+                topologyKey: "kubernetes.io/hostname"
+                namespaces:
+                  - "kube-system"
+        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
+        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.21.0-0
+        deployment:
+          enabled: true
+          replicas: 1
+      proxy:
+        enabled: true
+      service:
+        selector: false
+  - constraint: ">= 1.23.0-0 < v1.23.9-0"
+    values:
+      namespaceOverride: ""
+      clients:
+        enabled: true
+        port: 10015
+        useLocalhost: true
+        tolerations:
+          - effect: "NoExecute"
+            operator: "Exists"
+          - effect: "NoSchedule"
+            operator: "Exists"
+        affinity:
+          podAffinity:
+            requiredDuringSchedulingIgnoredDuringExecution:
+              - labelSelector:
+                  matchExpressions:
+                    - key: "app.kubernetes.io/component"
+                      operator: "In"
+                      values:
+                        - "controller"
+                topologyKey: "kubernetes.io/hostname"
+                namespaces:
+                  - "kube-system"
+        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
+        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.20.0-0
+        deployment:
+          enabled: true
+          replicas: 1
+      proxy:
+        enabled: true
+      service:
+        selector: false
+  - constraint: ">= 1.24.0-0 < v1.24.3-0"
+    values:
+      namespaceOverride: ""
+      clients:
+        enabled: true
+        port: 10015
+        useLocalhost: true
+        tolerations:
+          - effect: "NoExecute"
+            operator: "Exists"
+          - effect: "NoSchedule"
+            operator: "Exists"
+        affinity:
+          podAffinity:
+            requiredDuringSchedulingIgnoredDuringExecution:
+              - labelSelector:
+                  matchExpressions:
+                    - key: "app.kubernetes.io/component"
+                      operator: "In"
+                      values:
+                        - "controller"
+                topologyKey: "kubernetes.io/hostname"
+                namespaces:
+                  - "kube-system"
+        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
+        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.20.0-0
+        deployment:
+          enabled: true
+          replicas: 1
+      proxy:
+        enabled: true
+      service:
+        selector: false
+
+
+
+## Additional PushProx Monitoring
+## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
+##
+
+# hardenedKubelet can only be deployed if kubelet.enabled=true
+# If enabled, it replaces the ServiceMonitor deployed by the default kubelet option with a 
+# PushProx-based exporter that does not require a host port to be open to scrape metrics.
+hardenedKubelet:
+  enabled: false
+  metricsPort: 10250
+  component: kubelet
+  clients:
+    port: 10015
+    useLocalhost: true
+    https:
+      enabled: true
+      useServiceAccountCredentials: true
+      insecureSkipVerify: true
+    rbac:
+      additionalRules:
+      - nonResourceURLs: ["/metrics/cadvisor"]
+        verbs: ["get"]
+      - apiGroups: [""]
+        resources: ["nodes/metrics"]
+        verbs: ["get"]
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+  serviceMonitor:
+    endpoints:
+    - port: metrics
+      honorLabels: true
+      relabelings:
+      - sourceLabels: [__metrics_path__]
+        targetLabel: metrics_path
+    - port: metrics
+      path: /metrics/cadvisor
+      honorLabels: true
+      relabelings:
+      - sourceLabels: [__metrics_path__]
+        targetLabel: metrics_path
+    - port: metrics
+      path: /metrics/probes
+      honorLabels: true
+      relabelings:
+      - sourceLabels: [__metrics_path__]
+        targetLabel: metrics_path
+
+# hardenedNodeExporter can only be deployed if nodeExporter.enabled=true
+# If enabled, it replaces the ServiceMonitor deployed by the default nodeExporter with a 
+# PushProx-based exporter that does not require a host port to be open to scrape metrics.
+hardenedNodeExporter:
+  enabled: false
+  metricsPort: 9796
+  component: node-exporter
+  clients:
+    port: 10016
+    useLocalhost: true
+    tolerations:
+      - effect: "NoExecute"
+        operator: "Exists"
+      - effect: "NoSchedule"
+        operator: "Exists"
+
+## Upgrades
+upgrade:
+  ## Run upgrade scripts before an upgrade or rollback via a Job hook
+  enabled: true
+  ## Image to use to run the scripts
+  image:
+    repository: rancher/shell
+    tag: v0.2.1-rc.7
+
+## Rancher Monitoring
+##
+
+rancherMonitoring:
+  enabled: true
+
+  ## A namespaceSelector to identify the namespace to find the Rancher deployment
+  ##
+  namespaceSelector:
+    matchNames:
+    - cattle-system
+
+  ## A selector to identify the Rancher deployment
+  ## If not set, the chart will try to search for the Rancher deployment in the cattle-system namespace and infer the selector values from it
+  ## If the Rancher deployment does not exist, no resources will be deployed.
+  ##
+  selector: {}
+
+## Component scraping nginx-ingress-controller
+##
+ingressNginx:
+  enabled: false
+
+  ## The namespace to search for your nginx-ingress-controller
+  ##
+  namespace: ingress-nginx
+  
+  service:
+    port: 9913
+    targetPort: 10254
+    # selector:
+    #   app: ingress-nginx
+  serviceMonitor:
+    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
+    ##
+    interval: "30s"
+
+    ## proxyUrl: URL of a proxy that should be used for scraping.
+    ##
+    proxyUrl: ""
+
+    ## 	metric relabel configs to apply to samples before ingestion.
+    ##
+    metricRelabelings: []
+    # - action: keep
+    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
+    #   sourceLabels: [__name__]
+
+    # 	relabel configs to apply to samples before ingestion.
+    ##
+    relabelings: []
+    # - sourceLabels: [__meta_kubernetes_pod_node_name]
+    #   separator: ;
+    #   regex: ^(.*)$
+    #   targetLabel: nodename
+    #   replacement: $1
+    #   action: replace
+
+# Prometheus Operator Configuration
+
 ## Provide a name in place of kube-prometheus-stack for `app:` labels
+## NOTE: If you change this value, you must update the prometheus-adapter.prometheus.url
 ##
-nameOverride: ""
+nameOverride: "rancher-monitoring"
 
 ## Override the deployment namespace
+## NOTE: If you change this value, you must update the prometheus-adapter.prometheus.url
 ##
-namespaceOverride: ""
+namespaceOverride: "cattle-monitoring-system"
 
 ## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6
 ##
@@ -201,13 +823,37 @@
 
 ##
 global:
+  cattle:
+    psp:
+      enabled: false
+
+    systemDefaultRegistry: ""
+    ## Windows Monitoring
+    ## ref: https://github.com/rancher/charts/tree/dev-v2.5-source/packages/rancher-windows-exporter
+    ##
+    ## Deploys a DaemonSet of Prometheus exporters based on https://github.com/prometheus-community/windows_exporter.
+    ## Every Windows host must have a wins version of 0.1.0+ to use this chart (default as of Rancher 2.5.8).
+    ## To upgrade wins versions on Windows hosts, see https://github.com/rancher/wins/tree/master/charts/rancher-wins-upgrader.
+    ##
+    windows:
+      enabled: false
+  seLinux:
+    enabled: false
+  kubectl:
+     repository: rancher/kubectl
+     tag: v1.20.2
+     pullPolicy: IfNotPresent
   rbac:
+    ## Create RBAC resources for ServiceAccounts and users
+    ##
     create: true
 
-    ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs
-    ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
-    createAggregateClusterRoles: false
-    pspEnabled: false
+    userRoles:
+      ## Create default user ClusterRoles to allow users to interact with Prometheus CRs, ConfigMaps, and Secrets
+      create: true
+      ## Aggregate default user ClusterRoles into default k8s ClusterRoles
+      aggregateToDefaultRoles: true
+
     pspAnnotations: {}
       ## Specify pod annotations
       ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
@@ -220,7 +866,7 @@
 
   ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)
   ##
-  imageRegistry: ""
+  imageRegistry: docker.io
 
   ## Reference to one or more secrets to be used when pulling images
   ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
@@ -364,25 +1010,77 @@
   ## ref: https://prometheus.io/docs/alerting/notifications/
   ##      https://prometheus.io/docs/alerting/notification_examples/
   ##
-  templateFiles: {}
-  #
-  ## An example template:
-  #   template_1.tmpl: |-
-  #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
-  #
-  #       {{ define "slack.myorg.text" }}
-  #       {{- $root := . -}}
-  #       {{ range .Alerts }}
-  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
-  #         *Cluster:* {{ template "cluster" $root }}
-  #         *Description:* {{ .Annotations.description }}
-  #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
-  #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
-  #         *Details:*
-  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
-  #           {{ end }}
-  #       {{ end }}
-  #       {{ end }}
+
+  templateFiles:
+    rancher_defaults.tmpl: |-
+        {{- define "slack.rancher.text" -}}
+        {{ template "rancher.text_multiple" . }}
+        {{- end -}}
+
+        {{- define "rancher.text_multiple" -}}
+        *[GROUP - Details]*
+        One or more alarms in this group have triggered a notification.
+
+        {{- if gt (len .GroupLabels.Values) 0 }}
+        *Group Labels:*
+          {{- range .GroupLabels.SortedPairs }}
+          • *{{ .Name }}:* `{{ .Value }}`
+          {{- end }}
+        {{- end }}
+        {{- if .ExternalURL }}
+        *Link to AlertManager:* {{ .ExternalURL }}
+        {{- end }}
+
+        {{- range .Alerts }}
+        {{ template "rancher.text_single" . }}
+        {{- end }}
+        {{- end -}}
+
+        {{- define "rancher.text_single" -}}
+        {{- if .Labels.alertname }}
+        *[ALERT - {{ .Labels.alertname }}]*
+        {{- else }}
+        *[ALERT]*
+        {{- end }}
+        {{- if .Labels.severity }}
+        *Severity:* `{{ .Labels.severity }}`
+        {{- end }}
+        {{- if .Labels.cluster }}
+        *Cluster:*  {{ .Labels.cluster }}
+        {{- end }}
+        {{- if .Annotations.summary }}
+        *Summary:* {{ .Annotations.summary }}
+        {{- end }}
+        {{- if .Annotations.message }}
+        *Message:* {{ .Annotations.message }}
+        {{- end }}
+        {{- if .Annotations.description }}
+        *Description:* {{ .Annotations.description }}
+        {{- end }}
+        {{- if .Annotations.runbook_url }}
+        *Runbook URL:* <{{ .Annotations.runbook_url }}|:spiral_note_pad:>
+        {{- end }}
+        {{- with .Labels }}
+        {{- with .Remove (stringSlice "alertname" "severity" "cluster") }}
+        {{- if gt (len .) 0 }}
+        *Additional Labels:*
+          {{- range .SortedPairs }}
+          • *{{ .Name }}:* `{{ .Value }}`
+          {{- end }}
+        {{- end }}
+        {{- end }}
+        {{- end }}
+        {{- with .Annotations }}
+        {{- with .Remove (stringSlice "summary" "message" "description" "runbook_url") }}
+        {{- if gt (len .) 0 }}
+        *Additional Annotations:*
+          {{- range .SortedPairs }}
+          • *{{ .Name }}:* `{{ .Value }}`
+          {{- end }}
+        {{- end }}
+        {{- end }}
+        {{- end }}
+        {{- end -}}
 
   ingress:
     enabled: false
@@ -428,6 +1126,9 @@
   secret:
     annotations: {}
 
+  # by default the alertmanager secret is not overwritten if it already exists
+    recreateIfExists: false
+
   ## Configuration for creating an Ingress that will map to each Alertmanager replica service
   ## alertmanager.servicePerReplica must be enabled
   ##
@@ -646,8 +1347,7 @@
     ## Image of Alertmanager
     ##
     image:
-      registry: quay.io
-      repository: prometheus/alertmanager
+      repository: rancher/mirrored-prometheus-alertmanager
       tag: v0.27.0
       sha: ""
 
@@ -788,9 +1488,13 @@
     ## Define resources requests and limits for single Pods.
     ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
     ##
-    resources: {}
-    # requests:
-    #   memory: 400Mi
+    resources:
+      limits:
+        memory: 500Mi
+        cpu: 1000m
+      requests:
+        memory: 100Mi
+        cpu: 100m
 
     ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
     ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
@@ -946,6 +1650,30 @@
   enabled: true
   namespaceOverride: ""
 
+  ## Grafana's primary configuration
+  ## NOTE: values in map will be converted to ini format
+  ## ref: http://docs.grafana.org/installation/configuration/
+  ##
+  grafana.ini:
+    users:
+      auto_assign_org_role: Viewer
+    auth:
+      disable_login_form: false
+    auth.anonymous:
+      enabled: true
+      org_role: Viewer
+    auth.basic:
+      enabled: false
+    dashboards:
+      # Modify this value to change the default dashboard shown on the main Grafana page
+      default_home_dashboard_path: /tmp/dashboards/rancher-default-home.json
+    security:
+      # Required to embed dashboards in Rancher Cluster Overview Dashboard on Cluster Explorer
+      allow_embedding: true
+
+  deploymentStrategy:
+    type: Recreate
+
   ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
   ##
   forceDeployDatasources: false
@@ -958,6 +1686,18 @@
   ##
   defaultDashboardsEnabled: true
 
+  # Additional options for defaultDashboards
+  defaultDashboards:
+    # The default namespace to place defaultDashboards within
+    namespace: cattle-dashboards
+    # Whether to create the default namespace as a Helm managed namespace or use an existing namespace
+    # If false, the defaultDashboards.namespace will be created as a Helm managed namespace
+    useExistingNamespace: false
+    # Whether the Helm managed namespace created by this chart should be left behind on a Helm uninstall
+    # If you place other dashboards in this namespace, then they will be deleted on a helm uninstall
+    # Ignore if useExistingNamespace is true
+    cleanupOnUninstall: false
+
   ## Timezone for the default dashboards
   ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg
   ##
@@ -969,11 +1709,6 @@
 
   adminPassword: prom-operator
 
-  rbac:
-    ## If true, Grafana PSPs will be created
-    ##
-    pspEnabled: false
-
   ingress:
     ## If true, Grafana Ingress will be created
     ##
@@ -1032,9 +1767,8 @@
     dashboards:
       enabled: true
       label: grafana_dashboard
+      searchNamespace: cattle-dashboards
       labelValue: "1"
-      # Allow discovery in all namespaces for dashboards
-      searchNamespace: ALL
 
       # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels
       enableNewTablePanelSyntax: false
@@ -1121,8 +1855,63 @@
   ## Passed to grafana subchart and used by servicemonitor below
   ##
   service:
-    portName: http-web
+    portName: nginx-http
+    ## Port for Grafana Service to listen on
+    ##
+    port: 80
+    ## To be used with a proxy extraContainer port
+    ##
+    targetPort: 8080
+    ## Port to expose on each node
+    ## Only used if service.type is 'NodePort'
+    ##
+    nodePort: 30950
+    ## Service type
+    ##
+    type: ClusterIP
+
+  proxy:
+    image:
+      repository: rancher/mirrored-library-nginx
+      tag: 1.24.0-alpine
+
+  ## Enable an Specify container in extraContainers. This is meant to allow adding an authentication proxy to a grafana pod
+  extraContainers: |
+    - name: grafana-proxy
+      args:
+      - nginx
+      - -g
+      - daemon off;
+      - -c
+      - /nginx/nginx.conf
+      image: "{{ template "system_default_registry" . }}{{ .Values.proxy.image.repository }}:{{ .Values.proxy.image.tag }}"
+      ports:
+      - containerPort: 8080
+        name: nginx-http
+        protocol: TCP
+      volumeMounts:
+      - mountPath: /nginx
+        name: grafana-nginx
+      - mountPath: /var/cache/nginx
+        name: nginx-home
+      securityContext:
+        runAsUser: 101
+        runAsGroup: 101
 
+  ## Volumes that can be used in containers
+  extraContainerVolumes:
+    - name: nginx-home
+      emptyDir: {}
+    - name: grafana-nginx
+      configMap:
+        name: grafana-nginx-proxy-config
+        items:
+        - key: nginx.conf
+          mode: 438
+          path: nginx.conf
+
+  ## If true, create a serviceMonitor for grafana
+  ##
   serviceMonitor:
     # If true, a ServiceMonitor CRD is created for a prometheus operator
     # https://github.com/coreos/prometheus-operator
@@ -1156,6 +1945,17 @@
     #   replacement: $1
     #   action: replace
 
+  resources:
+    limits:
+      memory: 200Mi
+      cpu: 200m
+    requests:
+      memory: 100Mi
+      cpu: 100m
+
+  testFramework:
+    enabled: false
+
 ## Flag to disable all the kubernetes component scrapers
 ##
 kubernetesServiceMonitors:
@@ -1438,7 +2238,7 @@
 ## Component scraping the kube controller manager
 ##
 kubeControllerManager:
-  enabled: true
+  enabled: false
 
   ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
   ##
@@ -1700,7 +2500,7 @@
 ## Component scraping etcd
 ##
 kubeEtcd:
-  enabled: true
+  enabled: false
 
   ## If your etcd is not deployed as a pod, specify IPs it can be found on
   ##
@@ -1801,7 +2601,7 @@
 ## Component scraping kube scheduler
 ##
 kubeScheduler:
-  enabled: true
+  enabled: false
 
   ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
   ##
@@ -1899,7 +2699,7 @@
 ## Component scraping kube proxy
 ##
 kubeProxy:
-  enabled: true
+  enabled: false
 
   ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
   ##
@@ -2140,10 +2940,6 @@
       #   targetLabel: nodename
       #   replacement: $1
       #   action: replace
-  rbac:
-    ## If true, create PSPs for node-exporter
-    ##
-    pspEnabled: false
 
 ## Manages Prometheus and Alertmanager components
 ##
@@ -2167,8 +2963,8 @@
     enabled: true
     # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
     tlsMinVersion: VersionTLS13
-    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
-    internalPort: 10250
+    # Users who are deploying this chart in GKE private clusters will need to add firewall rules to expose this port for admissions webhooks
+    internalPort: 8443
 
   ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
   ## rules from making their way into prometheus and potentially preventing the container from starting
@@ -2298,9 +3094,9 @@
       ##
       image:
         registry: quay.io
-        repository: prometheus-operator/admission-webhook
+        repository: rancher/mirrored-prometheus-operator-admission-webhook
         # if not set appVersion field from Chart.yaml is used
-        tag: ""
+        tag: v0.72.0
         sha: ""
         pullPolicy: IfNotPresent
 
@@ -2409,8 +3205,7 @@
     patch:
       enabled: true
       image:
-        registry: registry.k8s.io
-        repository: ingress-nginx/kube-webhook-certgen
+        repository: rancher/mirrored-ingress-nginx-kube-webhook-certgen
         tag: v20221220-controller-v1.5.1-58-g787ea74b6
         sha: ""
         pullPolicy: IfNotPresent
@@ -2647,13 +3442,13 @@
 
   ## Resource limits & requests
   ##
-  resources: {}
-  # limits:
-  #   cpu: 200m
-  #   memory: 200Mi
-  # requests:
-  #   cpu: 100m
-  #   memory: 100Mi
+  resources:
+    limits:
+      cpu: 200m
+      memory: 500Mi
+    requests:
+      cpu: 100m
+      memory: 100Mi
 
   ## Operator Environment
   ##  env:
@@ -2755,10 +3550,8 @@
   ## Prometheus-operator image
   ##
   image:
-    registry: quay.io
-    repository: prometheus-operator/prometheus-operator
-    # if not set appVersion field from Chart.yaml is used
-    tag: ""
+    repository: rancher/mirrored-prometheus-operator-prometheus-operator
+    tag: v0.72.0
     sha: ""
     pullPolicy: IfNotPresent
 
@@ -2782,10 +3575,8 @@
   ##
   prometheusConfigReloader:
     image:
-      registry: quay.io
-      repository: prometheus-operator/prometheus-config-reloader
-      # if not set appVersion field from Chart.yaml is used
-      tag: ""
+      repository: rancher/mirrored-prometheus-operator-prometheus-config-reloader
+      tag: v0.72.0
       sha: ""
 
     # add prometheus config reloader liveness and readiness probe. Default: false
@@ -2803,8 +3594,7 @@
   ## Thanos side-car image when configured
   ##
   thanosImage:
-    registry: quay.io
-    repository: thanos/thanos
+    repository: rancher/mirrored-thanos-thanos
     tag: v0.34.1
     sha: ""
 
@@ -2988,7 +3778,7 @@
     port: 9090
 
     ## To be used with a proxy extraContainer port
-    targetPort: 9090
+    targetPort: 8081
 
     ## Port for Prometheus Reloader to listen on
     ##
@@ -3308,15 +4098,15 @@
     ## Defaults to 30s.
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
     ##
-    scrapeInterval: ""
+    scrapeInterval: "30s"
 
     ## Number of seconds to wait for target to respond before erroring
     ##
-    scrapeTimeout: ""
+    # scrapeTimeout: "30s"
 
     ## Interval between consecutive evaluations.
     ##
-    evaluationInterval: ""
+    evaluationInterval: "30s"
 
     ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
     ##
@@ -3352,8 +4142,7 @@
     ## Image of Prometheus.
     ##
     image:
-      registry: quay.io
-      repository: prometheus/prometheus
+      repository: rancher/mirrored-prometheus-prometheus
       tag: v2.50.1
       sha: ""
 
@@ -3453,7 +4242,7 @@
     ## prometheus resource to be created with selectors based on values in the helm deployment,
     ## which will also match the PrometheusRule resources created
     ##
-    ruleSelectorNilUsesHelmValues: true
+    ruleSelectorNilUsesHelmValues: false
 
     ## PrometheusRules to be selected for target discovery.
     ## If {}, select all PrometheusRules
@@ -3478,7 +4267,7 @@
     ## prometheus resource to be created with selectors based on values in the helm deployment,
     ## which will also match the servicemonitors created
     ##
-    serviceMonitorSelectorNilUsesHelmValues: true
+    serviceMonitorSelectorNilUsesHelmValues: false
 
     ## ServiceMonitors to be selected for target discovery.
     ## If {}, select all ServiceMonitors
@@ -3501,7 +4290,7 @@
     ## prometheus resource to be created with selectors based on values in the helm deployment,
     ## which will also match the podmonitors created
     ##
-    podMonitorSelectorNilUsesHelmValues: true
+    podMonitorSelectorNilUsesHelmValues: false
 
     ## PodMonitors to be selected for target discovery.
     ## If {}, select all PodMonitors
@@ -3663,9 +4452,13 @@
 
     ## Resource limits & requests
     ##
-    resources: {}
-    # requests:
-    #   memory: 400Mi
+    resources:
+      limits:
+        memory: 3000Mi
+        cpu: 1000m
+      requests:
+        memory: 750Mi
+        cpu: 750m
 
     ## Prometheus StorageSpec for persistent data
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
@@ -3688,7 +4481,13 @@
     #    medium: Memory
 
     # Additional volumes on the output StatefulSet definition.
-    volumes: []
+    volumes:
+      - name: nginx-home
+        emptyDir: {}
+      - name: prometheus-nginx
+        configMap:
+          name: prometheus-nginx-proxy-config
+          defaultMode: 438
 
     # Additional VolumeMounts on the output StatefulSet definition.
     volumeMounts: []
@@ -3852,25 +4651,34 @@
       #     #   access_key: ""
       #     #   secret_key: ""
 
+    proxy:
+      image:
+        repository: rancher/mirrored-library-nginx
+        tag: 1.24.0-alpine
+
     ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.
     ## if using proxy extraContainer update targetPort with proxy container port
-    containers: []
-    # containers:
-    # - name: oauth-proxy
-    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
-    #   args:
-    #   - --upstream=http://127.0.0.1:9090
-    #   - --http-address=0.0.0.0:8081
-    #   - --metrics-address=0.0.0.0:8082
-    #   - ...
-    #   ports:
-    #   - containerPort: 8081
-    #     name: oauth-proxy
-    #     protocol: TCP
-    #   - containerPort: 8082
-    #     name: oauth-metrics
-    #     protocol: TCP
-    #   resources: {}
+    containers:  |
+      - name: prometheus-proxy
+        args:
+        - nginx
+        - -g
+        - daemon off;
+        - -c
+        - /nginx/nginx.conf
+        image: "{{ template "system_default_registry" . }}{{ .Values.prometheus.prometheusSpec.proxy.image.repository }}:{{ .Values.prometheus.prometheusSpec.proxy.image.tag }}"
+        ports:
+        - containerPort: 8081
+          name: nginx-http
+          protocol: TCP
+        volumeMounts:
+        - mountPath: /nginx
+          name: prometheus-nginx
+        - mountPath: /var/cache/nginx
+          name: nginx-home
+        securityContext:
+          runAsUser: 101
+          runAsGroup: 101
 
     ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
     ## (permissions, dir tree) on mounted volumes before starting prometheus
@@ -3895,7 +4703,7 @@
     ## they will only discover targets within the namespace of the PodMonitor, ServiceMonitor and Probe object,
     ## and servicemonitors will be installed in the default service namespace.
     ## Defaults to false.
-    ignoreNamespaceSelectors: false
+    ignoreNamespaceSelectors: true
 
     ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.
     ## The label value will always be the namespace of the object that is being created.
@@ -4338,10 +5146,8 @@
     ## Image of ThanosRuler
     ##
     image:
-      registry: quay.io
-      repository: thanos/thanos
+      repository: rancher/mirrored-thanos-thanos
       tag: v0.34.1
-      sha: ""
 
     ## Namespaces to be selected for PrometheusRules discovery.
     ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
